{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;font-size:200%;;\">Vietnamese Sentiment Analyais </h2>\n",
    "<h3  style=\"text-align:center;\"><span class=\"label label-success\">NLP</span> <span class=\"label label-success\">Sentiment Analysis</span> <span class=\"label label-success\">Bag of Words</span> <span class=\"label label-success\">TF IDF</span> <span class=\"label label-success\">Basic Machine Learning</span> <span class=\"label label-success\"></span></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "[](http://)\n",
    "\n",
    "# **Table of Contents**\n",
    "\n",
    "\n",
    "1.\t[READ DATA](#1)\n",
    "2.\t[IMPLEMENT MODEL USING BAG OF WORDS](#2)\n",
    "3.  [Naive bayes bow](#3)\n",
    "4.\t[Logistic regression bow](#4)\n",
    "5.\t[Decision Tree bow](#5)    \n",
    "6.\t[KNN bow](#6)\n",
    "7.\t[SVM bow](#7)\n",
    "10.\t[IMPLEMENT MODEL USING TF-IDF](#8)\n",
    "11.\t[Naive bayes tf-idf](#9)\n",
    "12.\t[Logistic regression tf-idf](#10)\n",
    "13.\t[Decision Tree tf-idf](#11)\n",
    "14.\t[KNN tf-idf](#12)\n",
    "15.\t[SVM tf-idf](#13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tientd3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from pyvi) (0.23.2)\n",
      "Requirement already satisfied: six in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: tabulate in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.54.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from scikit-learn->pyvi) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from scikit-learn->pyvi) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tientd3\\vinbdi\\lib\\site-packages (from scikit-learn->pyvi) (0.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\tientd3\\vinbdi\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from pprint import pprint\n",
    "import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "!pip install pyvi\n",
    "from pyvi import ViTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Read data** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_valid = pd.read_csv('eval.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "# Read stopword\n",
    "df_stopword = pd.read_csv('vietnamese-stopwords-dash.txt', header=None)\n",
    "stopword = df_stopword.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train length distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49.543780303956325, 41.605379552354975, 378, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('df_train length distribution')\n",
    "len_train = df_train.comment.str.len()\n",
    "len_train.mean(), len_train.std(), len_train.max(), len_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_valid length distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48.27607561929596, 40.24405155635034, 323, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('df_valid length distribution')\n",
    "len_valid = df_valid.comment.str.len()\n",
    "len_valid.mean(), len_valid.std(), len_valid.max(), len_valid.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test length distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48.67731421121252, 41.112807906276714, 343, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('df_test length distribution')\n",
    "len_test = df_test.comment.str.len()\n",
    "len_test.mean(), len_test.std(), len_test.max(), len_test.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS    0.639368\n",
       "NEG    0.212769\n",
       "NEU    0.147863\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check labels percentage\n",
    "df_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vải đẹp  kiểu ih hình   rất hài lòng ' 'màn hình to và khá là hd '\n",
      " 'đóng gói sản phẩm rất đẹp và chắc chắn chất lượng sản phẩm tuyệt vời nhưng hơi không vừa vì hơi rộng '\n",
      " ...\n",
      " 'đẹp lắm  cửa hàng  ơi  mình mập cứ sợ không vừa  ai ngờ vừa y  cảm ơn  cửa hàng  '\n",
      " 'chất lượng sản phẩm tuyệt vời ' 'ko  được  sao nào hết ']\n",
      "0        2\n",
      "1        2\n",
      "2        2\n",
      "3        2\n",
      "4        2\n",
      "        ..\n",
      "24538    1\n",
      "24539    0\n",
      "24540    2\n",
      "24541    2\n",
      "24542    0\n",
      "Name: label, Length: 24543, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dic = {'NEG':0, 'NEU':1, 'POS':2}\n",
    "\n",
    "def convert_df(df, dic):\n",
    "    x = df.comment.values\n",
    "    y = df.label.map(dic)\n",
    "    return x, y\n",
    "\n",
    "xtrain, ytrain = convert_df(df_train, dic)\n",
    "xvalid, yvalid = convert_df(df_valid, dic)\n",
    "xtest, ytest = convert_df(df_test, dic)\n",
    "print(xtrain)\n",
    "print(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Implement Models Using Bag of words** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ctv = CountVectorizer(tokenizer = word_tokenize,stop_words=list(stopword),\n",
    "                       token_pattern = None)\n",
    "ctv.fit(xtrain)\n",
    "xtrain_transform, xvalid_transform, xtest_transform = ctv.transform(xtrain), ctv.transform(xvalid), ctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1. Naive bayes BOW** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive BS accuracy train\n",
      "0.7661247606242105\n",
      "Naive BS accuracy valid\n",
      "0.7330508474576272\n",
      "Naive BS accuracy test\n",
      "0.7340286831812256\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_naivebayes = MultinomialNB()\n",
    "model_naivebayes.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_naivebayes.predict(xtrain_transform)\n",
    "preds_valid = model_naivebayes.predict(xvalid_transform)\n",
    "preds_test =  model_naivebayes.predict(xtest_transform)\n",
    "print('Naive BS accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Naive BS accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Naive BS accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))\n",
    "\n",
    "Pkl_Filename = \"MultinomialNB_bow.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model_naivebayes, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Logistic regression BOW** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cách nhóm em tunning hyperparameters cho các mô hình :\n",
    "#### Sử dụng sự kết hợp giữa random search + gridsearch để tìm ra hyperparameter tốt \n",
    "#### Bước 1: Sử dụng random search để tìm kiếm trên diện rộng 1 cách ngẫu nhiên. Giá trị trả về là parameter tốt nhất theo random search.\n",
    "#### Bước 2: Từ giá trị trả về ở bước 1, ta biết được parameters cần tìm sẽ nằm ở khoảng lân cận gần đó. Lúc này dùng gridsearch để tìm kiếm trong các giá trị lân cận này"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7548385838229701\n",
      "Best parameters set: \n",
      "\tC: 0.3593813663804626\n",
      "\tclass_weight: None\n",
      "\tdual: False\n",
      "\tfit_intercept: True\n",
      "\tintercept_scaling: 1\n",
      "\tl1_ratio: None\n",
      "\tmax_iter: 100\n",
      "\tmulti_class: auto\n",
      "\tn_jobs: -1\n",
      "\tpenalty: l2\n",
      "\trandom_state: None\n",
      "\tsolver: lbfgs\n",
      "\ttol: 0.0001\n",
      "\tverbose: 0\n",
      "\twarm_start: False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# Tuning C penalty\n",
    "# Random search\n",
    "c = np.logspace(-4,4,10)\n",
    "grid_cv =    {\n",
    "               'C': c,\n",
    "               }\n",
    "\n",
    "lr = linear_model.LogisticRegression(n_jobs=-1, penalty='l2')  # solver: saga\n",
    "\n",
    "model =RandomizedSearchCV(     estimator = lr, \n",
    "                               param_distributions= grid_cv, \n",
    "                               scoring='accuracy',\n",
    "                               cv = 5,\n",
    "                               n_jobs = -1\n",
    "                    )\n",
    "\n",
    "model.fit(xtrain_transform, ytrain)\n",
    "\n",
    "print(f'Best score: {model.best_score_}')\n",
    "print(f'Best parameters set: ')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for pagram in best_parameters.keys():\n",
    "    print(f'\\t{pagram}: {best_parameters[pagram]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7551237078811551\n",
      "Best parameters set: \n",
      "\tC: 0.2\n",
      "\tclass_weight: None\n",
      "\tdual: False\n",
      "\tfit_intercept: True\n",
      "\tintercept_scaling: 1\n",
      "\tl1_ratio: None\n",
      "\tmax_iter: 100\n",
      "\tmulti_class: auto\n",
      "\tn_jobs: -1\n",
      "\tpenalty: l2\n",
      "\trandom_state: None\n",
      "\tsolver: lbfgs\n",
      "\ttol: 0.0001\n",
      "\tverbose: 0\n",
      "\twarm_start: False\n"
     ]
    }
   ],
   "source": [
    "# After we see C = 0.36, gridsearch\n",
    "c = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "grid_cv =    {\n",
    "               'C': c,\n",
    "               }\n",
    "\n",
    "lr = linear_model.LogisticRegression(n_jobs=-1, penalty='l2')  # solver: saga\n",
    "\n",
    "model = GridSearchCV(          estimator = lr, \n",
    "                               param_grid = grid_cv, \n",
    "                               scoring='accuracy',\n",
    "                               cv = 5,\n",
    "                               n_jobs = -1\n",
    "                    )\n",
    "\n",
    "model.fit(xtrain_transform, ytrain)\n",
    "\n",
    "print(f'Best score: {model.best_score_}')\n",
    "print(f'Best parameters set: ')\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for pagram in best_parameters.keys():\n",
    "    print(f'\\t{pagram}: {best_parameters[pagram]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy train\n",
      "0.7891455812247892\n",
      "Logistic accuracy valid\n",
      "0.7571707953063885\n",
      "Logistic accuracy test\n",
      "0.7542372881355932\n"
     ]
    }
   ],
   "source": [
    "# After using randomsearch --> gridsearch , set the hyparameter c = 0.2 as follow\n",
    "model_logistic = linear_model.LogisticRegression(penalty='l2', max_iter=200, C=0.2)\n",
    "model_logistic.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_logistic.predict(xtrain_transform)\n",
    "preds_valid = model_logistic.predict(xvalid_transform)\n",
    "preds_test = model_logistic.predict(xtest_transform)\n",
    "print('Logistic accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Logistic accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Logistic accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))\n",
    "\n",
    "Pkl_Filename = \"Logistic_cbow.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model_logistic, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Decision Tree BOW** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy train\n",
      "0.8671311575602004\n",
      "Decision tree accuracy valid\n",
      "0.7050195567144719\n",
      "Decision tree accuracy test\n",
      "0.7131681877444589\n"
     ]
    }
   ],
   "source": [
    "model_tree = tree.DecisionTreeClassifier(max_depth=60)\n",
    "model_tree.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_tree.predict(xtrain_transform)\n",
    "preds_valid = model_tree.predict(xvalid_transform)\n",
    "preds_test = model_tree.predict(xtest_transform)\n",
    "print('Decision tree accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Decision tree accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Decision tree accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))\n",
    "\n",
    "Pkl_Filename = \"Decision_bow.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model_tree, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4. KNN bow** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy train\n",
      "0.7250539868801695\n",
      "KNN accuracy valid\n",
      "0.7102346805736637\n",
      "KNN accuracy test\n",
      "0.7167535853976532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier( n_neighbors = 19,\n",
    "                              p = 2,\n",
    "                              weights = 'uniform', n_jobs = -1)\n",
    "\n",
    "model_knn.fit(xtrain_transform, ytrain)\n",
    "preds = model_knn.predict(xvalid_transform)\n",
    "preds_train = model_knn.predict(xtrain_transform)\n",
    "preds_valid = model_knn.predict(xvalid_transform)\n",
    "preds_test = model_knn.predict(xtest_transform)\n",
    "print('KNN accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('KNN accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('KNN accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))\n",
    "\n",
    "Pkl_Filename = \"KNN_tfidf.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(model_knn, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5. SVM bow** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy train\n",
      "0.8675793505276453\n",
      "SVM accuracy valid\n",
      "0.7679269882659713\n",
      "SVM accuracy test\n",
      "0.7636897001303781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svm = SVC(C=2)\n",
    "model_svm.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_svm.predict(xtrain_transform)\n",
    "preds_valid = model_svm.predict(xvalid_transform)\n",
    "preds_test = model_svm.predict(xtest_transform)\n",
    "print('SVM accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('SVM accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('SVM accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Implement model using TF-IDF** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ctv = TfidfVectorizer(  max_features=5000\n",
    "                      , ngram_range=(1,3)\n",
    "                      , tokenizer=ViTokenizer.tokenize\n",
    "                      , stop_words=list(stopword))\n",
    "                                                            \n",
    "xtrain, ytrain = convert_df(df_train, dic)\n",
    "xvalid, yvalid = convert_df(df_valid, dic)\n",
    "xtest, ytest = convert_df(df_test, dic)\n",
    "\n",
    "ctv.fit(xtrain)\n",
    "xtrain_transform, xvalid_transform, xtest_transform = ctv.transform(xtrain), ctv.transform(xvalid), ctv.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Naive bayes TFIDF** <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive BS accuracy train\n",
      "0.7532901438291978\n",
      "Naive BS accuracy valid\n",
      "0.7291395045632334\n",
      "Naive BS accuracy test\n",
      "0.7438070404172099\n"
     ]
    }
   ],
   "source": [
    "model_naivebayes = MultinomialNB()\n",
    "model_naivebayes.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_naivebayes.predict(xtrain_transform)\n",
    "preds_valid = model_naivebayes.predict(xvalid_transform)\n",
    "preds_test =  model_naivebayes.predict(xtest_transform)\n",
    "print('Naive BS accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Naive BS accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Naive BS accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Logistic regression TFIDF** <a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy train\n",
      "0.809110540683698\n",
      "Logistic accuracy valid\n",
      "0.7809647979139505\n",
      "Logistic accuracy test\n",
      "0.7897653194263363\n"
     ]
    }
   ],
   "source": [
    "model_logistic = linear_model.LogisticRegression(penalty='l2', max_iter=200, C=0.6)\n",
    "model_logistic.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_logistic.predict(xtrain_transform)\n",
    "preds_valid = model_logistic.predict(xvalid_transform)\n",
    "preds_test = model_logistic.predict(xtest_transform)\n",
    "print('Logistic accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Logistic accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Logistic accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Decision Tree TFIDF** <a class=\"anchor\" id=\"11\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy train\n",
      "0.976897689768977\n",
      "Decision tree accuracy valid\n",
      "0.7050195567144719\n",
      "Decision tree accuracy test\n",
      "0.720013037809648\n"
     ]
    }
   ],
   "source": [
    "model_tree = tree.DecisionTreeClassifier(max_depth=60)\n",
    "model_tree.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_tree.predict(xtrain_transform)\n",
    "preds_valid = model_tree.predict(xvalid_transform)\n",
    "preds_test = model_tree.predict(xtest_transform)\n",
    "print('Decision tree accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('Decision tree accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('Decision tree accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 KNN TFIDF** <a class=\"anchor\" id=\"12\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy train\n",
      "0.7839709896915618\n",
      "KNN accuracy valid\n",
      "0.7561929595827901\n",
      "KNN accuracy test\n",
      "0.7715123859191656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier( n_neighbors = 19,\n",
    "                              p = 2,\n",
    "                              weights = 'uniform', n_jobs = -1)\n",
    "\n",
    "model_knn.fit(xtrain_transform, ytrain)\n",
    "preds = model_knn.predict(xvalid_transform)\n",
    "preds_train = model_knn.predict(xtrain_transform)\n",
    "preds_valid = model_knn.predict(xvalid_transform)\n",
    "preds_test = model_knn.predict(xtest_transform)\n",
    "print('KNN accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('KNN accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('KNN accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5 SVM TFIDF** <a class=\"anchor\" id=\"13\"></a>\n",
    "\n",
    "[Go back to table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy train\n",
      "0.9507395183962841\n",
      "SVM accuracy valid\n",
      "0.7868318122555411\n",
      "SVM accuracy test\n",
      "0.8008474576271186\n"
     ]
    }
   ],
   "source": [
    "model_svm = SVC(C=2)\n",
    "model_svm.fit(xtrain_transform, ytrain)\n",
    "preds_train = model_svm.predict(xtrain_transform)\n",
    "preds_valid = model_svm.predict(xvalid_transform)\n",
    "preds_test = model_svm.predict(xtest_transform)\n",
    "print('SVM accuracy train')\n",
    "print(metrics.accuracy_score(ytrain, preds_train))\n",
    "print('SVM accuracy valid')\n",
    "print(metrics.accuracy_score(yvalid, preds_valid))\n",
    "print('SVM accuracy test')\n",
    "print(metrics.accuracy_score(ytest, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
